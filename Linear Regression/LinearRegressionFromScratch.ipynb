{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the linear regression model:\n",
    "$ y=b_{0}+b_{1}x $ .\n",
    "\n",
    "The optimalization algorithm relies on traditional gradient-finding.\n",
    "I have implemend two method of fitting the linear regression. The first is described above, the second is Adaptive Moment (Adam) optimalization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.25404805,  1.41196348]), 0.08011607540401058)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def MSE(X: np.ndarray, y: np.ndarray, b0: float, b1:float) -> float:\n",
    "    \"\"\"The function computes the MeanSquareError for the linear regression model\"\"\"\n",
    "\n",
    "    return np.sum((b1*X + b0 - y)**2)/X.shape[0]\n",
    "\n",
    "     \n",
    "\n",
    "def FitTheLineV1(dataset:pl.DataFrame, predictor:str, target_var:str, eps = 0.001) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"The function finds the optimal parameters, b0, b1, so that \n",
    "    the loss function of linear regressionmodel dataset[target_var] = b0 + b1*dataset[predictor] is minimized\"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    B = np.random.uniform(low = -3, high = 3, size = 2)\n",
    "\n",
    "    #Take the X feature and y feature from the dataset. Then, numpyize it!. Afterward, normalize it!\n",
    "    MinMaxScaler_X = MinMaxScaler()\n",
    "    MinMaxScaler_y = MinMaxScaler()\n",
    "\n",
    "    X = MinMaxScaler_X.fit_transform(dataset.select(predictor).to_numpy())\n",
    "    y = MinMaxScaler_y.fit_transform(dataset.select(target_var).to_numpy())\n",
    "\n",
    "\n",
    "    #Define the h value for computing derivatives and learning-rate (lr) for updating b0, b1 parameters.\n",
    "    h = 0.001\n",
    "    lr = 0.01\n",
    "    #The following variable indicates the difference between the new value of the function and the old one.\n",
    "    #If it's  absolute is less than eps, break the loop.\n",
    "    val_change= float('inf')\n",
    "\n",
    "    while val_change  >= eps:\n",
    "        #Compute the gradients for b0 and b1 variables respectively.\n",
    "        prev_MSE_val= MSE(X, y, B[0], B[1])\n",
    "\n",
    "        grad_b0 = (MSE(X, y,  B[0]+h, B[1]) - prev_MSE_val)/h\n",
    "        grad_b1 = (MSE(X, y,  B[0], B[1]+h) - prev_MSE_val)/h\n",
    "\n",
    "        gradient = np.array([grad_b0, grad_b1])\n",
    "\n",
    "        B = B - lr * gradient\n",
    "\n",
    "        curr_MSE_val = MSE(X, y, B[0], B[1])\n",
    "        val_change = abs(curr_MSE_val - prev_MSE_val)\n",
    "\n",
    "\n",
    "    return B, MSE(X, y, B[0],B[1])\n",
    "\n",
    "        \n",
    "\n",
    "#Read the dataset, from which we take the features.\n",
    "dataset = pl.read_csv(\"WineQuality.csv\", separator=';', ignore_errors = True)\n",
    "\n",
    "\n",
    "#Select the features for the model.\n",
    "target_var = \"quality\"\n",
    "predictor = \"pH\"\n",
    "\n",
    "\n",
    "print(FitTheLineV1(dataset, predictor, target_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimalization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.32986797,  1.48994695]), 0.09721925361635286)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class Adam():\n",
    "        \n",
    "    #Note, we assume obj_func takes two, equally-shaped, one-dimensional arrays and two parameters: b0 and b1 (thus n_params = 2)\n",
    "    def __init__(self,  params:np.ndarray,  X: np.ndarray, y: np.ndarray, alpha:float = 0.001, eps: float = 10**(-8)):\n",
    "        assert X.shape[0] == y.shape[0], \"The first dimensions of the arrays should match!\"\n",
    "\n",
    "        assert type(alpha) is float and alpha > 0, \"The alpha parameter needs to be a positive, numerical value.\"\n",
    "\n",
    "    \n",
    "        self.params = params\n",
    "        self.n_params = params.shape[0]\n",
    "        \n",
    "        #Declare the X variable and y variable.\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Define the alpha and eps which stands for: learning-rate and zero-division-preventer respectively.\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "        #Declare the initial state of m and v (first-moment gradient, second-moment gradient).\n",
    "        self.m = np.zeros(self.n_params)\n",
    "        self.v = np.zeros(self.n_params)\n",
    "\n",
    "    def MSE(self, b0: float, b1:float) -> float:\n",
    "        return np.sum((b1*self.X + b0 - self.y)**2)/self.X.shape[0]\n",
    "\n",
    "\n",
    "    def FindGradients(self,h:float) -> np.ndarray:\n",
    "        partial_derivs = np.zeros(self.n_params)\n",
    "        substractor = self.MSE(self.params[0], self.params[1])\n",
    "\n",
    "        partial_derivs[0] = (self.MSE(self.params[0]+h, self.params[1]) - substractor)/h\n",
    "        partial_derivs[1] = (self.MSE(self.params[0], self.params[1]+h) - substractor)/h\n",
    "\n",
    "        return partial_derivs\n",
    "\n",
    "\n",
    "    \n",
    "    def UpdateMoments(self, bet1:float, bet2:float, h:float):\n",
    "        Gradients = self.FindGradients(h)\n",
    "\n",
    "        new_m = (bet1*self.m + (1-bet1)*Gradients)/(1-bet1)\n",
    "        new_v = (bet2*self.v + (1-bet2)*Gradients**2)/(1-bet2)\n",
    "\n",
    "\n",
    "        return new_m, new_v\n",
    "    \n",
    "    def UpdateParameters(self, bet1:float, bet2:float, h:float):\n",
    "        m_corr, v_corr = self.UpdateMoments(bet1, bet2, h)\n",
    "\n",
    "        self.params = self.params - self.alpha * m_corr / ( np.sqrt(v_corr) +self.eps)\n",
    "\n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def FitTheLineV2(dataset:pl.DataFrame, predictor:str, target_var:str, eps = 0.001) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"The function finds the optimal parameters, b0, b1, so that \n",
    "    the loss function of linear regressionmodel dataset[target_var] = b0 + b1*dataset[predictor] is minimized\"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    B = np.random.uniform(low = -3, high = 3, size = 2)\n",
    "\n",
    "    #Take the X feature and y feature from the dataset. Then, numpyize it!. Afterward, normalize it!\n",
    "    MinMaxScaler_X = MinMaxScaler()\n",
    "    MinMaxScaler_y = MinMaxScaler()\n",
    "\n",
    "    X = MinMaxScaler_X.fit_transform(dataset.select(predictor).to_numpy())\n",
    "    y = MinMaxScaler_y.fit_transform(dataset.select(target_var).to_numpy())\n",
    "\n",
    "\n",
    "    #Declare an instance of the Adam optimizer.\n",
    "    Adamek = Adam(B, X, y)\n",
    "\n",
    "    #The following variable indicates the difference between the new value of the function and the old one.\n",
    "    #If it's  absolute is less than eps, break the loop.\n",
    "    val_change= float('inf')\n",
    "\n",
    "    while val_change  >= eps:\n",
    "        B_new = Adamek.UpdateParameters(0.9, 0.9, 0.001)\n",
    "\n",
    "        val_change = abs( Adamek.MSE(B_new[0], B_new[1]) - Adamek.MSE(B[0], B[1]))\n",
    "        B = B_new\n",
    "\n",
    "\n",
    "\n",
    "    return B, Adamek.MSE(B[0], B[1])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "FitTheLineV2(dataset, predictor, target_var, eps = 0.0005)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
