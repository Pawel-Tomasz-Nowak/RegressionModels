{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the needed packages.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# def Variance(feature:np.ndarray, q:float, nrepeat:int = 20):\n",
    "\n",
    "#     samples = np.random.choice(feature, size = (nrepeat,int(feature.shape[0]*q)),)\n",
    "\n",
    "#     return np.mean(samples.var(axis=1))\n",
    "\n",
    "\n",
    "class LinearRegression():\n",
    "   \n",
    "    def __init__(self, Dataset: np.ndarray, nfeat: int, tar_id: int,  dx:float, μ:float, eps:float, random_state:int = -1):\n",
    "        \"\"\"The LinearRegression class needs a dataset it will be learning from. \\\\\n",
    "            integer \"nfeat\" is a number of features we wanna include in the model. Obviously nfeat <= Dataset.shape[1].\n",
    "            dx is a small, positive floating-point number for computing the derivatives of the optimizer.\n",
    "            μ is so-called learning rate of the training-process.\n",
    "            tar_id is the index of the target variable.\n",
    "            Eps is, in turn, a division-by-zero preventing value when updating the values of the parameters.\"\"\"\n",
    "        \n",
    "        if random_state == -1:\n",
    "            self.train_set, self.test_set = train_test_split(Dataset, train_size = 0.8)\n",
    "        else:\n",
    "            self.train_set, self.test_set = train_test_split(Dataset, train_size = 0.8, random_state = random_state)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "        self.nfeat = nfeat\n",
    "\n",
    "        self.tar_id = tar_id\n",
    "        self.dx = dx\n",
    "        self.μ = μ\n",
    "        self.eps = eps\n",
    "        #Define the array of feature-indeces we wanna include in the model. In the end, len(sel_features) == f (True) \n",
    "        self.sel_feats = np.array([], dtype = np.int16)\n",
    "        self.sel_params = np.array([], dtype = np.float64)\n",
    "\n",
    "    #Preprocess the features before feature selections\n",
    "    def FeaturePreprocessing(self, dataset:np.ndarray):\n",
    "        #Declare the ThresholdVariance object\n",
    "        ThresholdVariance = VarianceThreshold()\n",
    "        dataset = ThresholdVariance.fit_transform(dataset)\n",
    "\n",
    "        #Declare the MinMaxScaler object.\n",
    "        Normalizator = MinMaxScaler()\n",
    "        #Scale the dataset to [0;1] interval.\n",
    "        dataset = Normalizator.fit_transform(dataset)\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "\n",
    "    #The following dataset will be a subset of the main Dataset.\n",
    "    def MSE(self, dataset:np.ndarray, predictors:np.ndarray, params:np.ndarray,) -> float:\n",
    "        \"\"\"Computes the square root of sum of squared errors between predicted y and actual y values\"\"\"\n",
    "        score = np.sum(((dataset[:, predictors] * params[1:] + params[0]).sum(axis=1) - dataset[:, self.tar_id])**2)\n",
    "\n",
    "        return np.sqrt(score)\n",
    "    \n",
    "\n",
    "    def ComputeGradient(self, dataset:np.ndarray, predictors:np.ndarray, params:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"The method finds the gradient (that is - the array of the values of the derivatives of all predictors)\"\"\"\n",
    "        Gradient = np.zeros(params.shape[0])\n",
    "\n",
    "        substractor = self.MSE(dataset, predictors, params)\n",
    "\n",
    "        for i in range(params.shape[0]):\n",
    "            mod_params = params.copy()\n",
    "            mod_params[i] += self.dx\n",
    "\n",
    "            Gradient[i] = (self.MSE(dataset, predictors, mod_params) - substractor)/self.dx\n",
    "            \n",
    "\n",
    "        return Gradient\n",
    "\n",
    "\n",
    "    def UpdateParameters(self, dataset:np.ndarray, predictors:np.ndarray, Bm:float, Bv:float) -> np.ndarray:\n",
    "        \"\"\"The aim of the function is to find the set of parameters so that the MSE value is minimilized\"\"\"\n",
    "        params = np.ones(shape = predictors.shape[0]+1)\n",
    "        #Define the first momentum of the gradient.\n",
    "        m = np.zeros(params.shape[0])\n",
    "        #Define the second momentum of the gradient\n",
    "        v = np.zeros(params.shape[0])\n",
    "\n",
    "\n",
    "        for _ in range(1000):\n",
    "            #Find the Gradient of the objective function:\n",
    "            Gradient = self.ComputeGradient(dataset, predictors, params)\n",
    "\n",
    "            #Update the values of the m.\n",
    "            m = Bm*m + (1-Bm)*Gradient\n",
    "            #Correct the value of the m\n",
    "            m = m/(1-Bm)\n",
    "\n",
    "\n",
    "            #Update the values of the v.\n",
    "            v = Bv*v + (1-Bv)*Gradient**2\n",
    "            #Correct the value of the v\n",
    "            v = v/(1-Bv)\n",
    "\n",
    "\n",
    "            #Update the parameters\n",
    "            params = params - m*self.μ/(np.sqrt(v) + self.eps)\n",
    "\n",
    "\n",
    "        return params\n",
    "        \n",
    "    #Find the score for the set of parameters.\n",
    "    def FindTheScore(self, dataset:np.ndarray, predictors:np.ndarray) -> float:\n",
    "        fitted_params = self.UpdateParameters(dataset, predictors, 0.3, 0.3)\n",
    "        score = self.MSE(dataset, predictors, fitted_params)\n",
    "\n",
    "    \n",
    "        return (score, fitted_params)\n",
    "    \n",
    "    def SelectFeatures(self):\n",
    "        dataset = self.FeaturePreprocessing(self.train_set)\n",
    "\n",
    "\n",
    "        #--------If the scores decrease slightly, break the loop. Change it!-------\n",
    "\n",
    "        while self.sel_feats.shape[0] < self.nfeat:\n",
    "            #Initialize the variables which will store the optimal feature.\n",
    "            best_feat_id:int = None\n",
    "            best_feat_score :float= float('inf')\n",
    "            best_params: np.ndarray = np.array([])\n",
    "\n",
    "            is_change:bool = False\n",
    "\n",
    "            for i in range(dataset.shape[1]):\n",
    "                if i!=self.tar_id and i not in self.sel_feats:\n",
    "                    sel_feats_test = np.append(self.sel_feats, i)\n",
    "\n",
    "                    score, params = self.FindTheScore(dataset, sel_feats_test)\n",
    "\n",
    "                    #If the  feature has lower score or equal score and greater variance,  select the feature.\n",
    "                    #-----If scales of two different features are not the same, variance can be deceptive.------\n",
    "                    if score < best_feat_score or (score == best_feat_score and np.var(dataset[:, i]) > np.var(dataset[:,best_feat_id ])):\n",
    "                        best_feat_id = i\n",
    "                        best_feat_score = score\n",
    "                        best_params = params\n",
    "\n",
    "                        is_change = True\n",
    "\n",
    "            #If the features has been selected, append it to the set.\n",
    "            if is_change:\n",
    "                self.sel_feats = np.append(self.sel_feats, [best_feat_id])\n",
    "                self.sel_params = best_params\n",
    "            \n",
    "        #Compute the final score.\n",
    "        final_score  = self.MSE(self.test_set, self.sel_feats, self.sel_params)\n",
    "        return self.sel_feats, self.sel_params, final_score\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Sprawdź na wykresie, jak liczba cech wpływa na błąd oraz jak losowy podział danych wpływa na dokładność.\n",
    "#Porównaj rozwiązanie analityczne oraz numeryczne rozwiązania.\n",
    "#Analityczne rozwiązanie istnieje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of accuracies for different numbers of included featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '13.24,2.59,2.87,21,118,2.8,2.69,0.39,1.82,4.32,1.04,2.93,735,0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nfeats \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,nfeats\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     15\u001b[0m     LinReg \u001b[38;5;241m=\u001b[39m LinearRegression(Dataset\u001b[38;5;241m.\u001b[39mto_numpy(), nfeats, Dataset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     Scores[nfeats\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mLinReg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelectFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     18\u001b[0m     Scores[nfeats\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m nfeats\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(Scores[:,\u001b[38;5;241m0\u001b[39m],Scores[:, \u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 120\u001b[0m, in \u001b[0;36mLinearRegression.SelectFeatures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSelectFeatures\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeaturePreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m#--------If the scores decrease slightly, break the loop. Change it!-------\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msel_feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfeat:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m#Initialize the variables which will store the optimal feature.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mLinearRegression.FeaturePreprocessing\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFeaturePreprocessing\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset:np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#Declare the ThresholdVariance object\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     ThresholdVariance \u001b[38;5;241m=\u001b[39m VarianceThreshold()\n\u001b[1;32m---> 46\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mThresholdVariance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#Declare the MinMaxScaler object.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     Normalizator \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_variance_threshold.py:99\u001b[0m, in \u001b[0;36mVarianceThreshold.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn empirical variances from X.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# sparse matrix\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariances_ \u001b[38;5;241m=\u001b[39m mean_variance_axis(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '13.24,2.59,2.87,21,118,2.8,2.69,0.39,1.82,4.32,1.04,2.93,735,0'"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "Dataset = pl.read_csv(\"wine_data.csv\", separator = ';', ignore_errors = True).drop_nulls()\n",
    "#Define a two-feature dataset. The first feature is a number of features included, the second - the corresponding score.\n",
    "\n",
    "nfeats = 5\n",
    "\n",
    "Scores = np.zeros(shape = [nfeats, 2])\n",
    "\n",
    "\n",
    "for nfeats in range(1,nfeats+1):\n",
    "    LinReg = LinearRegression(Dataset.to_numpy(), nfeats, Dataset.shape[1]-2, 0.001, 0.001, 0.001)\n",
    "\n",
    "    Scores[nfeats-1, 1] = LinReg.SelectFeatures()[2]\n",
    "    Scores[nfeats-1, 0] = nfeats\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(Scores[:,0],Scores[:, 1])\n",
    "print(Scores)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie stabilności rozwiązania (Badanie dokładności modelu w zależności od zbioru treningowegon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.9203906 14.7821934 20.6798331]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 25\n",
    "\n",
    "\n",
    "\n",
    "Scores2 = np.zeros(shape = [n, 2],)\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    LinReg_inst = LinearRegression(Dataset.to_numpy(), 1, Dataset.shape[1]-1, 0.001, 0.001, 0.001)\n",
    "    Scores2[i, :] = [i,LinReg_inst.SelectFeatures()[2]]\n",
    "\n",
    "    \n",
    "Scores2\n",
    "\n",
    "\n",
    "plt.boxplot(x = Scores2[:, 1])\n",
    "print(np.quantile(Scores2[:, 1], q  = [0.25, 0.5, 0.75]))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
